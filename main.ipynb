{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf582744",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a239603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import ByteLevel\n",
    "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    PreTrainedTokenizerFast,\n",
    "    GPT2Config, GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments, Trainer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3c2cc5",
   "metadata": {},
   "source": [
    "### Model Configs\n",
    "Setting up the dataset, Tokenizer, and model.\n",
    "\n",
    "The dataset for this project is the full text of Moby Dick. It is spilt into two portions, a training and a validation text file.\n",
    "\n",
    "Using a GPT2 architecture model with 8 heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cfac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Train a tokenizer (Byte-level BPE) ----------\n",
    "data_dir = Path(\"./data\") \n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "files = [str(data_dir/\"train.txt\")]\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer.pre_tokenizer = ByteLevel()\n",
    "tokenizer.decoder = ByteLevelDecoder()\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=50257,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"]\n",
    ")\n",
    "tokenizer.train(files=files, trainer=trainer)\n",
    "tokenizer.save(\"tokenizer.json\")\n",
    "\n",
    "hf_tok = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"tokenizer.json\",\n",
    "    bos_token=\"<s>\", eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\", pad_token=\"<pad>\"\n",
    ")\n",
    "hf_tok.save_pretrained(\"./my-tokenizer\")\n",
    "\n",
    "# ---------- Build a GPT-style model from scratch ----------\n",
    "config = GPT2Config(\n",
    "    vocab_size=hf_tok.vocab_size,\n",
    "    n_positions=1024,\n",
    "    n_ctx=1024,\n",
    "    n_embd=512,\n",
    "    n_layer=6,\n",
    "    n_head=8,\n",
    "    bos_token_id=hf_tok.bos_token_id,\n",
    "    eos_token_id=hf_tok.eos_token_id\n",
    ")\n",
    "model = GPT2LMHeadModel(config)\n",
    "model.resize_token_embeddings(hf_tok.vocab_size)\n",
    "\n",
    "# ---------- Load text dataset and tokenize ----------\n",
    "raw = load_dataset(\"text\", data_files={\n",
    "    \"train\": str(data_dir/\"train.txt\"),\n",
    "    \"validation\": str(data_dir/\"val.txt\")\n",
    "})\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return hf_tok(batch[\"text\"])\n",
    "\n",
    "tokenized = raw.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# group into fixed-length blocks for causal LM\n",
    "block_size = 1024\n",
    "def group_texts(examples):\n",
    "    # concatenate\n",
    "    concat = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_len = (len(concat[\"input_ids\"]) // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "        for k, t in concat.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "lm_ds = tokenized.map(group_texts, batched=True)\n",
    "\n",
    "# ensure pad/bos/eos are aligned everywhere\n",
    "model.config.pad_token_id = hf_tok.pad_token_id\n",
    "model.config.bos_token_id = hf_tok.bos_token_id\n",
    "model.config.eos_token_id = hf_tok.eos_token_id\n",
    "\n",
    "# generation config too\n",
    "model.generation_config.pad_token_id = hf_tok.pad_token_id\n",
    "model.generation_config.bos_token_id = hf_tok.bos_token_id\n",
    "model.generation_config.eos_token_id = hf_tok.eos_token_id\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.loss_type =\"ForCausalLMLoss\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79806d86",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41a129c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 02:36, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>8.731400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 925.7794436520039\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./my-tiny-gpt\\\\tokenizer_config.json',\n",
       " './my-tiny-gpt\\\\special_tokens_map.json',\n",
       " './my-tiny-gpt\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------- Train ----------\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=hf_tok, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-4,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=2000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=10,\n",
    "    fp16=True,                        # set bf16=True on Ampere+/TPUs if available\n",
    "    gradient_checkpointing=True,      # helps memory\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=lm_ds[\"train\"],\n",
    "    eval_dataset=lm_ds[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=hf_tok\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ---------- Evaluate perplexity ----------\n",
    "eval_res = trainer.evaluate()\n",
    "print(\"Perplexity:\", math.exp(eval_res[\"eval_loss\"]))\n",
    "\n",
    "# ---------- Save ----------\n",
    "trainer.save_model(\"./my-tiny-gpt\")\n",
    "hf_tok.save_pretrained(\"./my-tiny-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7477eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, now over the Pequod its,, the sea.! them, and a; and to it of the whale, and the most, the. And now, and the with the’s, and I in the. But, and so,\n"
     ]
    }
   ],
   "source": [
    "# ---------- Quick generation test ----------\n",
    "prompt = \"Once upon a time\"\n",
    "inputs = hf_tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "gen_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(hf_tok.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88a647",
   "metadata": {},
   "source": [
    "### Loading model\n",
    "\n",
    "Trained model utilizing google colab GPU resources. Loading that model for better results. The model does not produce valid sentences, however, it captures the style and feel of the book. Given the limited data and model size, these results are impressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8f8315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, and over cried out upon which I must?”“Come out a good fare about all eagerness to ye grin at thy black black vomit Ahab. I stood in a leg.When the skin of the insane’s too I got, I know him\n"
     ]
    }
   ],
   "source": [
    "# --- paths ---\n",
    "checkpoint_dir = \"./best_model\"   # change to your checkpoint-* dir\n",
    "\n",
    "# --- load model & tokenizer from checkpoint ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)\n",
    "\n",
    "# GPT2-like models may not have pad token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "gen_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.95\n",
    ")\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
